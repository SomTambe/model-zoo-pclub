{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final3DGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xV3CyYZRN3_",
        "colab_type": "code",
        "outputId": "6abb5cf3-587b-4ebd-bf4a-d88d2e376fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed May 20 08:59:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz97V9UmK4n2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets\n",
        "import torch.utils.data as loader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchsummary import summary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grYKbJSuQ-Zy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import gzip\n",
        "import os\n",
        "class ModelNet10GAN(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for training a 3D-GAN, without using a variational autoencoder.\n",
        "    I have not implemented download of dataset. Also keep a minimum RAM of 12 GB.\n",
        "    \"\"\"\n",
        "    def __init__(self, dir=\"./\", download=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dir (string): Path in which you want the dataset \n",
        "                to be saved (keep a minimum space of 9 GBs).\n",
        "            download (boolean): Set to True if you want to download. \n",
        "                Default=False.\n",
        "        \"\"\"\n",
        "        self.dir=dir\n",
        "        self.download=download\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError('chair.npy.gz not present in '+self.dir)\n",
        "        with gzip.open(self.dir+'chair.npy.gz','rb') as f:\n",
        "            self.arr=np.load(f)\n",
        "        \n",
        "        \n",
        " \n",
        "    def __getitem__(self, ind):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            ind (int): Index of the sample you want. \n",
        " \n",
        "        Returns:\n",
        "            Tensor: (torch.Tensor, Size: (1,64,64,64))\n",
        "        \"\"\"\n",
        "        return torch.tensor(self.arr[ind+1])\n",
        " \n",
        "    def __len__(self):\n",
        "        return (self.arr.shape[0]-1)\n",
        " \n",
        "    def _check_exists(self):\n",
        "        return (os.path.exists(self.dir+\"chair.npy.gz\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLjztjzgRIob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Storing device\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "# Defining ModelNet10 dataset for GAN\n",
        "dataset=ModelNet10GAN(dir='./')\n",
        "# Defining batch-size for every iteration.\n",
        "batchsize=64\n",
        "# Loading dataset into dataloader\n",
        "data_loader=loader.DataLoader(dataset, batch_size=batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b57SH-IUQZqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# This is the input vector size. Default = 200 (as per paper)\n",
        "vectorSize=200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsGG9X6gQdsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generator for 3D-GAN-\n",
        "    The generator consists of five fully convolution layers with numbers of channels\n",
        "    {512, 256, 128, 64, 1}, kernel sizes {4, 4, 4, 4, 4}, and strides {1, 2, 2, 2, 2}. \n",
        "    We add ReLU and batch normalization layers between convolutional layers, and a Sigmoid\n",
        "    layer at the end. The input is a 200-dimensional vector, and the output is a 64 × 64 × 64 \n",
        "    matrix with values in [0, 1].\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1=nn.Sequential(\n",
        "            nn.ConvTranspose3d(in_channels=200,out_channels=512,kernel_size=2,stride=1,bias=False),\n",
        "            nn.BatchNorm3d(512),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.layer2=nn.Sequential(\n",
        "            nn.ConvTranspose3d(in_channels=512,out_channels=256,kernel_size=2,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.layer3=nn.Sequential(\n",
        "            nn.ConvTranspose3d(in_channels=256,out_channels=128,kernel_size=2,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.layer4=nn.Sequential(\n",
        "            nn.ConvTranspose3d(in_channels=128,out_channels=64,kernel_size=2,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "        self.layer5=nn.Sequential(\n",
        "            nn.ConvTranspose3d(in_channels=64,out_channels=1,kernel_size=2,stride=2,bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    \n",
        "    def forward(self,x):\n",
        "        bsize=x.size(0)\n",
        "        x=x.view(bsize,vectorSize,1,1,1)\n",
        "        x=self.layer1(x)\n",
        "        x=self.layer2(x)\n",
        "        x=self.layer3(x)\n",
        "        x=self.layer4(x)\n",
        "        x=self.layer5(x)\n",
        "        return x\n",
        "G_=generator().to(device)\n",
        "class discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator for 3D-GAN-\n",
        "    As a mirrored version of the generator, the discriminator takes as input a \n",
        "    64 × 64 × 64 matrix, and outputs a real number in [0, 1]. The discriminator \n",
        "    consists of 5 volumetric convolution layers, with numbers of channels {64,128,256,512,1}, \n",
        "    kernel sizes {4,4,4,4,4}, and strides {2, 2, 2, 2, 1}. There are leaky ReLU layers of \n",
        "    parameter 0.2 and batch normalization layers in between, and a Sigmoid layer at the end.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1=nn.Sequential(\n",
        "            nn.Conv3d(in_channels=1,out_channels=64,kernel_size=3,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(64),\n",
        "            nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.layer2=nn.Sequential(\n",
        "            nn.Conv3d(in_channels=64,out_channels=128,kernel_size=3,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(128),\n",
        "            nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.layer3=nn.Sequential(\n",
        "            nn.Conv3d(in_channels=128,out_channels=256,kernel_size=3,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(256),\n",
        "            nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.layer4=nn.Sequential(\n",
        "            nn.Conv3d(in_channels=256,out_channels=512,kernel_size=3,stride=2,bias=False),\n",
        "            nn.BatchNorm3d(512),\n",
        "            nn.LeakyReLU(0.2,inplace=True)\n",
        "        )\n",
        "        self.layer5=nn.Sequential(\n",
        "            nn.Conv3d(in_channels=512,out_channels=1,kernel_size=1,stride=1,bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        " \n",
        "    def forward(self,x):\n",
        "        bsize=x.size(0)\n",
        "        x=self.layer1(x)\n",
        "        x=self.layer2(x)\n",
        "        x=self.layer3(x)\n",
        "        x=self.layer4(x)\n",
        "        x=self.layer5(x)\n",
        "        x=x.view(bsize,1)\n",
        "        return x\n",
        "D_=discriminator().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDXpRGH-Qla_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Number of epochs you want it to train for.\n",
        "num_epochs=100\n",
        "# If you want to continue training from perv. saved model, uncomment the lines below.\n",
        "# G_.load_state_dict(torch.load('./gen'))\n",
        "# D_.load_state_dict(torch.load('./dis'))\n",
        "\n",
        "# Defining the optimizers, Adam, with lr as specified in the paper.\n",
        "optimizerD=optim.Adam(D_.parameters(),lr=1e-6,betas=(0.5,0.999))\n",
        "optimizerG=optim.Adam(G_.parameters(),lr=0.0025,betas=(0.5,0.999))\n",
        "# Loss criterion as Binary Cross-Entropy Loss.\n",
        "criterion=nn.BCELoss()\n",
        "# Defining real and fake labels.\n",
        "real_label=1\n",
        "fake_label=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAk5GDQRWf92",
        "colab_type": "code",
        "outputId": "39599c43-e807-4073-b819-358ed7b875ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Arrays to store d_losses and g_losses.\n",
        "G_losses=[]\n",
        "D_losses=[]\n",
        "iters = 0\n",
        "k=10\n",
        "print(\"Starting training loop...\")\n",
        "# For every epoch.\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in epoch.\n",
        "    RunningLossG=0\n",
        "    RunningLossD=0\n",
        "    for i,data in enumerate(data_loader,1):\n",
        "        ############################\n",
        "        # Updating Discriminator : maximize log(D(x)) + log(1-D(G(z)))\n",
        "        ############################\n",
        "        for j in range(k):\n",
        "            D_.zero_grad()\n",
        "            ## Training with all real batch\n",
        "            real_data = data.to(device).float()\n",
        "            btSize=real_data.size(0)\n",
        "            label = torch.full((btSize,),real_label,device=device)\n",
        "            # Forward pass for real_batch.\n",
        "            output = D_(real_data).view(-1)\n",
        "            # Loss calculation for real_batch.\n",
        "            errD_real = criterion(output,label)\n",
        "            # Calculating gradients\n",
        "            errD_real.backward()\n",
        "\n",
        "            ## Training with all fake batch\n",
        "            # Generating batch of random noise vectors\n",
        "            noise = torch.randn((btSize, vectorSize), device=device)\n",
        "            # print(noise.shape)\n",
        "            # Generating fake models with G\n",
        "            fake = G_(noise)\n",
        "            label.fill_(fake_label)\n",
        "            output = D_(fake.detach()).view(-1)\n",
        "            errD_fake=criterion(output,label)\n",
        "            errD_fake.backward()\n",
        "            D_G_z1=output.mean().item()\n",
        "            errD=errD_real+errD_fake\n",
        "            # Update D network.\n",
        "            optimizerD.step()\n",
        "        output = D_(real_data).view(-1)\n",
        "        D_x = output.mean().item()\n",
        "        ############################\n",
        "        # Updating Generator : minimize -log(D(G(z)))\n",
        "        ############################\n",
        "        # label.fill_(real_label) # To get the log(x) part rather than log(1-x).\n",
        "        label.fill_(fake_label)\n",
        "        G_.zero_grad()\n",
        "        output=D_(fake).view(-1)\n",
        "        errG=-criterion(output,label)\n",
        "        errG.backward()\n",
        "        D_G_z2=output.mean().item()\n",
        "        # Update G network\n",
        "        optimizerG.step()\n",
        "        RunningLossD+=errD.item()\n",
        "        RunningLossG+=errG.item()\n",
        "        if i%7==0:\n",
        "            print('[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x):%.4f D(G(z)): %.4f/%.4f'%(epoch+1,num_epochs,(errD/k).item(),errG.item(),D_x,D_G_z1,D_G_z2))\n",
        "    G_losses.append(RunningLossG)\n",
        "    D_losses.append(RunningLossD/k)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting training loop...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/pytorch/aten/src/ATen/native/TensorFactories.cpp:361: UserWarning: Deprecation warning: In a future PyTorch release torch.full will no longer return tensors of floating dtype by default. Instead, a bool fill_value will return a tensor of torch.bool dtype, and an integral fill_value will return a tensor of torch.long dtype. Set the optional `dtype` or `out` arguments to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[1/100] Loss_D: 0.2915 Loss_G: -2.2921 D(x):0.5482 D(G(z)): 0.5677/0.5677\n",
            "[1/100] Loss_D: 0.3529 Loss_G: -2.9187 D(x):0.5510 D(G(z)): 0.5533/0.5533\n",
            "[2/100] Loss_D: 0.3731 Loss_G: -3.1361 D(x):0.5553 D(G(z)): 0.5051/0.5051\n",
            "[2/100] Loss_D: 0.3297 Loss_G: -2.6882 D(x):0.5595 D(G(z)): 0.4636/0.4639\n",
            "[3/100] Loss_D: 0.2411 Loss_G: -1.7932 D(x):0.5595 D(G(z)): 0.5213/0.5216\n",
            "[3/100] Loss_D: 0.2731 Loss_G: -2.1226 D(x):0.5637 D(G(z)): 0.5002/0.5000\n",
            "[4/100] Loss_D: 0.2971 Loss_G: -2.3658 D(x):0.5625 D(G(z)): 0.5085/0.5085\n",
            "[4/100] Loss_D: 0.2490 Loss_G: -1.8802 D(x):0.5669 D(G(z)): 0.5117/0.5118\n",
            "[5/100] Loss_D: 0.2422 Loss_G: -1.8144 D(x):0.5647 D(G(z)): 0.5180/0.5184\n",
            "[5/100] Loss_D: 0.2363 Loss_G: -1.7553 D(x):0.5695 D(G(z)): 0.5552/0.5549\n",
            "[6/100] Loss_D: 0.2065 Loss_G: -1.4611 D(x):0.5665 D(G(z)): 0.5274/0.5273\n",
            "[6/100] Loss_D: 0.2036 Loss_G: -1.4488 D(x):0.5718 D(G(z)): 0.5514/0.5514\n",
            "[7/100] Loss_D: 0.2045 Loss_G: -1.4441 D(x):0.5683 D(G(z)): 0.5116/0.5124\n",
            "[7/100] Loss_D: 0.2001 Loss_G: -1.4143 D(x):0.5738 D(G(z)): 0.5394/0.5401\n",
            "[8/100] Loss_D: 0.1859 Loss_G: -1.2738 D(x):0.5700 D(G(z)): 0.5284/0.5292\n",
            "[8/100] Loss_D: 0.1690 Loss_G: -1.1124 D(x):0.5758 D(G(z)): 0.5530/0.5533\n",
            "[9/100] Loss_D: 0.1759 Loss_G: -1.1747 D(x):0.5717 D(G(z)): 0.5609/0.5606\n",
            "[9/100] Loss_D: 0.1793 Loss_G: -1.2192 D(x):0.5778 D(G(z)): 0.5700/0.5694\n",
            "[10/100] Loss_D: 0.1669 Loss_G: -1.0864 D(x):0.5733 D(G(z)): 0.5753/0.5742\n",
            "[10/100] Loss_D: 0.1660 Loss_G: -1.0887 D(x):0.5794 D(G(z)): 0.5737/0.5732\n",
            "[11/100] Loss_D: 0.1708 Loss_G: -1.1320 D(x):0.5745 D(G(z)): 0.5623/0.5620\n",
            "[11/100] Loss_D: 0.1720 Loss_G: -1.1515 D(x):0.5807 D(G(z)): 0.5524/0.5523\n",
            "[12/100] Loss_D: 0.1690 Loss_G: -1.1123 D(x):0.5752 D(G(z)): 0.5650/0.5640\n",
            "[12/100] Loss_D: 0.1679 Loss_G: -1.1104 D(x):0.5818 D(G(z)): 0.5468/0.5470\n",
            "[13/100] Loss_D: 0.1710 Loss_G: -1.1315 D(x):0.5759 D(G(z)): 0.5412/0.5407\n",
            "[13/100] Loss_D: 0.1696 Loss_G: -1.1260 D(x):0.5829 D(G(z)): 0.5270/0.5268\n",
            "[14/100] Loss_D: 0.1852 Loss_G: -1.2802 D(x):0.5765 D(G(z)): 0.5313/0.5313\n",
            "[14/100] Loss_D: 0.1877 Loss_G: -1.3132 D(x):0.5838 D(G(z)): 0.5431/0.5429\n",
            "[15/100] Loss_D: 0.1828 Loss_G: -1.2551 D(x):0.5770 D(G(z)): 0.5116/0.5126\n",
            "[15/100] Loss_D: 0.1722 Loss_G: -1.1567 D(x):0.5849 D(G(z)): 0.5376/0.5372\n",
            "[16/100] Loss_D: 0.1885 Loss_G: -1.3113 D(x):0.5777 D(G(z)): 0.5605/0.5599\n",
            "[16/100] Loss_D: 0.1730 Loss_G: -1.1661 D(x):0.5858 D(G(z)): 0.5759/0.5751\n",
            "[17/100] Loss_D: 0.1794 Loss_G: -1.2221 D(x):0.5785 D(G(z)): 0.5484/0.5486\n",
            "[17/100] Loss_D: 0.1780 Loss_G: -1.2237 D(x):0.5867 D(G(z)): 0.5084/0.5086\n",
            "[18/100] Loss_D: 0.1753 Loss_G: -1.1837 D(x):0.5792 D(G(z)): 0.5537/0.5535\n",
            "[18/100] Loss_D: 0.1617 Loss_G: -1.0613 D(x):0.5876 D(G(z)): 0.5425/0.5423\n",
            "[19/100] Loss_D: 0.1784 Loss_G: -1.2167 D(x):0.5798 D(G(z)): 0.5519/0.5517\n",
            "[19/100] Loss_D: 0.1757 Loss_G: -1.2032 D(x):0.5883 D(G(z)): 0.5371/0.5374\n",
            "[20/100] Loss_D: 0.1697 Loss_G: -1.1237 D(x):0.5804 D(G(z)): 0.5357/0.5358\n",
            "[20/100] Loss_D: 0.1776 Loss_G: -1.2186 D(x):0.5889 D(G(z)): 0.5174/0.5174\n",
            "[21/100] Loss_D: 0.1855 Loss_G: -1.2865 D(x):0.5808 D(G(z)): 0.5686/0.5679\n",
            "[21/100] Loss_D: 0.1892 Loss_G: -1.3409 D(x):0.5896 D(G(z)): 0.5606/0.5600\n",
            "[22/100] Loss_D: 0.1747 Loss_G: -1.1772 D(x):0.5813 D(G(z)): 0.5545/0.5542\n",
            "[22/100] Loss_D: 0.1762 Loss_G: -1.2141 D(x):0.5900 D(G(z)): 0.5431/0.5432\n",
            "[23/100] Loss_D: 0.1733 Loss_G: -1.1664 D(x):0.5818 D(G(z)): 0.5745/0.5732\n",
            "[23/100] Loss_D: 0.1709 Loss_G: -1.1622 D(x):0.5905 D(G(z)): 0.5752/0.5743\n",
            "[24/100] Loss_D: 0.1710 Loss_G: -1.1397 D(x):0.5823 D(G(z)): 0.5695/0.5679\n",
            "[24/100] Loss_D: 0.1796 Loss_G: -1.2499 D(x):0.5910 D(G(z)): 0.5315/0.5317\n",
            "[25/100] Loss_D: 0.1927 Loss_G: -1.3654 D(x):0.5825 D(G(z)): 0.5431/0.5433\n",
            "[25/100] Loss_D: 0.1741 Loss_G: -1.1894 D(x):0.5912 D(G(z)): 0.5172/0.5182\n",
            "[26/100] Loss_D: 0.1872 Loss_G: -1.3104 D(x):0.5825 D(G(z)): 0.5536/0.5533\n",
            "[26/100] Loss_D: 0.1785 Loss_G: -1.2313 D(x):0.5911 D(G(z)): 0.5275/0.5274\n",
            "[27/100] Loss_D: 0.1730 Loss_G: -1.1636 D(x):0.5824 D(G(z)): 0.5502/0.5497\n",
            "[27/100] Loss_D: 0.1742 Loss_G: -1.1805 D(x):0.5913 D(G(z)): 0.5300/0.5299\n",
            "[28/100] Loss_D: 0.1738 Loss_G: -1.1697 D(x):0.5823 D(G(z)): 0.5195/0.5198\n",
            "[28/100] Loss_D: 0.1797 Loss_G: -1.2483 D(x):0.5915 D(G(z)): 0.5559/0.5549\n",
            "[29/100] Loss_D: 0.1795 Loss_G: -1.2325 D(x):0.5823 D(G(z)): 0.5439/0.5437\n",
            "[29/100] Loss_D: 0.1695 Loss_G: -1.1478 D(x):0.5917 D(G(z)): 0.5123/0.5127\n",
            "[30/100] Loss_D: 0.1690 Loss_G: -1.1257 D(x):0.5824 D(G(z)): 0.5049/0.5059\n",
            "[30/100] Loss_D: 0.1659 Loss_G: -1.1088 D(x):0.5921 D(G(z)): 0.5078/0.5085\n",
            "[31/100] Loss_D: 0.1722 Loss_G: -1.1607 D(x):0.5827 D(G(z)): 0.5345/0.5342\n",
            "[31/100] Loss_D: 0.1677 Loss_G: -1.1316 D(x):0.5925 D(G(z)): 0.5420/0.5414\n",
            "[32/100] Loss_D: 0.1728 Loss_G: -1.1643 D(x):0.5831 D(G(z)): 0.5353/0.5351\n",
            "[32/100] Loss_D: 0.1703 Loss_G: -1.1589 D(x):0.5928 D(G(z)): 0.5236/0.5238\n",
            "[33/100] Loss_D: 0.1799 Loss_G: -1.2302 D(x):0.5832 D(G(z)): 0.5657/0.5647\n",
            "[33/100] Loss_D: 0.1758 Loss_G: -1.2104 D(x):0.5931 D(G(z)): 0.5248/0.5243\n",
            "[34/100] Loss_D: 0.1726 Loss_G: -1.1626 D(x):0.5832 D(G(z)): 0.5527/0.5518\n",
            "[34/100] Loss_D: 0.1756 Loss_G: -1.2070 D(x):0.5931 D(G(z)): 0.5698/0.5682\n",
            "[35/100] Loss_D: 0.1703 Loss_G: -1.1345 D(x):0.5830 D(G(z)): 0.5447/0.5437\n",
            "[35/100] Loss_D: 0.1754 Loss_G: -1.2093 D(x):0.5932 D(G(z)): 0.5192/0.5191\n",
            "[36/100] Loss_D: 0.1770 Loss_G: -1.2048 D(x):0.5827 D(G(z)): 0.5286/0.5290\n",
            "[36/100] Loss_D: 0.1793 Loss_G: -1.2492 D(x):0.5937 D(G(z)): 0.5119/0.5118\n",
            "[37/100] Loss_D: 0.1853 Loss_G: -1.2920 D(x):0.5829 D(G(z)): 0.4808/0.4806\n",
            "[37/100] Loss_D: 0.1736 Loss_G: -1.1898 D(x):0.5940 D(G(z)): 0.5257/0.5250\n",
            "[38/100] Loss_D: 0.1692 Loss_G: -1.1260 D(x):0.5830 D(G(z)): 0.5447/0.5444\n",
            "[38/100] Loss_D: 0.1622 Loss_G: -1.0820 D(x):0.5942 D(G(z)): 0.5668/0.5656\n",
            "[39/100] Loss_D: 0.1658 Loss_G: -1.0944 D(x):0.5832 D(G(z)): 0.5674/0.5660\n",
            "[39/100] Loss_D: 0.1653 Loss_G: -1.1118 D(x):0.5941 D(G(z)): 0.5753/0.5738\n",
            "[40/100] Loss_D: 0.1657 Loss_G: -1.0982 D(x):0.5829 D(G(z)): 0.5723/0.5709\n",
            "[40/100] Loss_D: 0.1705 Loss_G: -1.1628 D(x):0.5938 D(G(z)): 0.6035/0.6015\n",
            "[41/100] Loss_D: 0.1640 Loss_G: -1.0774 D(x):0.5824 D(G(z)): 0.5484/0.5473\n",
            "[41/100] Loss_D: 0.1772 Loss_G: -1.2299 D(x):0.5934 D(G(z)): 0.5620/0.5613\n",
            "[42/100] Loss_D: 0.1655 Loss_G: -1.0917 D(x):0.5821 D(G(z)): 0.5305/0.5301\n",
            "[42/100] Loss_D: 0.1667 Loss_G: -1.1201 D(x):0.5931 D(G(z)): 0.5000/0.4997\n",
            "[43/100] Loss_D: 0.1641 Loss_G: -1.0747 D(x):0.5818 D(G(z)): 0.5271/0.5267\n",
            "[43/100] Loss_D: 0.1664 Loss_G: -1.1186 D(x):0.5929 D(G(z)): 0.5146/0.5142\n",
            "[44/100] Loss_D: 0.1686 Loss_G: -1.1197 D(x):0.5816 D(G(z)): 0.5028/0.5033\n",
            "[44/100] Loss_D: 0.1636 Loss_G: -1.0911 D(x):0.5930 D(G(z)): 0.5164/0.5164\n",
            "[45/100] Loss_D: 0.1666 Loss_G: -1.1019 D(x):0.5814 D(G(z)): 0.5163/0.5165\n",
            "[45/100] Loss_D: 0.1706 Loss_G: -1.1559 D(x):0.5932 D(G(z)): 0.5481/0.5472\n",
            "[46/100] Loss_D: 0.1707 Loss_G: -1.1458 D(x):0.5815 D(G(z)): 0.5285/0.5277\n",
            "[46/100] Loss_D: 0.1693 Loss_G: -1.1419 D(x):0.5935 D(G(z)): 0.5398/0.5385\n",
            "[47/100] Loss_D: 0.1676 Loss_G: -1.1177 D(x):0.5815 D(G(z)): 0.5546/0.5543\n",
            "[47/100] Loss_D: 0.1768 Loss_G: -1.2263 D(x):0.5938 D(G(z)): 0.5688/0.5675\n",
            "[48/100] Loss_D: 0.1666 Loss_G: -1.1079 D(x):0.5817 D(G(z)): 0.5736/0.5721\n",
            "[48/100] Loss_D: 0.1632 Loss_G: -1.0906 D(x):0.5940 D(G(z)): 0.5291/0.5280\n",
            "[49/100] Loss_D: 0.1692 Loss_G: -1.1240 D(x):0.5817 D(G(z)): 0.5407/0.5402\n",
            "[49/100] Loss_D: 0.1593 Loss_G: -1.0508 D(x):0.5939 D(G(z)): 0.5460/0.5447\n",
            "[50/100] Loss_D: 0.1746 Loss_G: -1.1826 D(x):0.5814 D(G(z)): 0.5668/0.5660\n",
            "[50/100] Loss_D: 0.1713 Loss_G: -1.1743 D(x):0.5939 D(G(z)): 0.5725/0.5715\n",
            "[51/100] Loss_D: 0.1658 Loss_G: -1.0958 D(x):0.5809 D(G(z)): 0.5409/0.5401\n",
            "[51/100] Loss_D: 0.1655 Loss_G: -1.1132 D(x):0.5936 D(G(z)): 0.5320/0.5321\n",
            "[52/100] Loss_D: 0.1717 Loss_G: -1.1486 D(x):0.5806 D(G(z)): 0.5139/0.5143\n",
            "[52/100] Loss_D: 0.1614 Loss_G: -1.0731 D(x):0.5936 D(G(z)): 0.5251/0.5248\n",
            "[53/100] Loss_D: 0.1711 Loss_G: -1.1503 D(x):0.5805 D(G(z)): 0.5651/0.5647\n",
            "[53/100] Loss_D: 0.1550 Loss_G: -1.0068 D(x):0.5938 D(G(z)): 0.5474/0.5464\n",
            "[54/100] Loss_D: 0.1633 Loss_G: -1.0681 D(x):0.5803 D(G(z)): 0.5536/0.5521\n",
            "[54/100] Loss_D: 0.1565 Loss_G: -1.0261 D(x):0.5935 D(G(z)): 0.5540/0.5528\n",
            "[55/100] Loss_D: 0.1632 Loss_G: -1.0657 D(x):0.5798 D(G(z)): 0.5670/0.5656\n",
            "[55/100] Loss_D: 0.1578 Loss_G: -1.0404 D(x):0.5932 D(G(z)): 0.5437/0.5432\n",
            "[56/100] Loss_D: 0.1694 Loss_G: -1.1282 D(x):0.5793 D(G(z)): 0.5622/0.5612\n",
            "[56/100] Loss_D: 0.1607 Loss_G: -1.0671 D(x):0.5930 D(G(z)): 0.5055/0.5051\n",
            "[57/100] Loss_D: 0.1661 Loss_G: -1.0976 D(x):0.5790 D(G(z)): 0.5396/0.5386\n",
            "[57/100] Loss_D: 0.1683 Loss_G: -1.1395 D(x):0.5929 D(G(z)): 0.5398/0.5396\n",
            "[58/100] Loss_D: 0.1752 Loss_G: -1.1856 D(x):0.5789 D(G(z)): 0.5462/0.5451\n",
            "[58/100] Loss_D: 0.1705 Loss_G: -1.1512 D(x):0.5929 D(G(z)): 0.5352/0.5340\n",
            "[59/100] Loss_D: 0.1651 Loss_G: -1.0776 D(x):0.5787 D(G(z)): 0.5661/0.5637\n",
            "[59/100] Loss_D: 0.1606 Loss_G: -1.0581 D(x):0.5928 D(G(z)): 0.5281/0.5263\n",
            "[60/100] Loss_D: 0.1751 Loss_G: -1.1814 D(x):0.5783 D(G(z)): 0.5416/0.5400\n",
            "[60/100] Loss_D: 0.1701 Loss_G: -1.1552 D(x):0.5925 D(G(z)): 0.5541/0.5529\n",
            "[61/100] Loss_D: 0.1648 Loss_G: -1.0758 D(x):0.5783 D(G(z)): 0.5127/0.5126\n",
            "[61/100] Loss_D: 0.1703 Loss_G: -1.1612 D(x):0.5926 D(G(z)): 0.5353/0.5348\n",
            "[62/100] Loss_D: 0.1702 Loss_G: -1.1265 D(x):0.5785 D(G(z)): 0.5496/0.5477\n",
            "[62/100] Loss_D: 0.1653 Loss_G: -1.1099 D(x):0.5931 D(G(z)): 0.5351/0.5336\n",
            "[63/100] Loss_D: 0.1700 Loss_G: -1.1313 D(x):0.5790 D(G(z)): 0.5146/0.5137\n",
            "[63/100] Loss_D: 0.1530 Loss_G: -0.9883 D(x):0.5935 D(G(z)): 0.5274/0.5258\n",
            "[64/100] Loss_D: 0.1665 Loss_G: -1.1009 D(x):0.5793 D(G(z)): 0.5121/0.5114\n",
            "[64/100] Loss_D: 0.1581 Loss_G: -1.0358 D(x):0.5936 D(G(z)): 0.5045/0.5041\n",
            "[65/100] Loss_D: 0.1616 Loss_G: -1.0476 D(x):0.5795 D(G(z)): 0.5411/0.5397\n",
            "[65/100] Loss_D: 0.1573 Loss_G: -1.0329 D(x):0.5938 D(G(z)): 0.5170/0.5171\n",
            "[66/100] Loss_D: 0.1564 Loss_G: -0.9990 D(x):0.5794 D(G(z)): 0.5345/0.5335\n",
            "[66/100] Loss_D: 0.1664 Loss_G: -1.1155 D(x):0.5939 D(G(z)): 0.5528/0.5511\n",
            "[67/100] Loss_D: 0.1703 Loss_G: -1.1396 D(x):0.5792 D(G(z)): 0.5505/0.5494\n",
            "[67/100] Loss_D: 0.1559 Loss_G: -1.0136 D(x):0.5940 D(G(z)): 0.5387/0.5363\n",
            "[68/100] Loss_D: 0.1652 Loss_G: -1.0854 D(x):0.5790 D(G(z)): 0.5518/0.5505\n",
            "[68/100] Loss_D: 0.1637 Loss_G: -1.0973 D(x):0.5940 D(G(z)): 0.5879/0.5859\n",
            "[69/100] Loss_D: 0.1673 Loss_G: -1.1037 D(x):0.5786 D(G(z)): 0.5686/0.5666\n",
            "[69/100] Loss_D: 0.1536 Loss_G: -0.9946 D(x):0.5937 D(G(z)): 0.5636/0.5610\n",
            "[70/100] Loss_D: 0.1679 Loss_G: -1.1061 D(x):0.5781 D(G(z)): 0.5764/0.5741\n",
            "[70/100] Loss_D: 0.1656 Loss_G: -1.1196 D(x):0.5932 D(G(z)): 0.5887/0.5873\n",
            "[71/100] Loss_D: 0.1620 Loss_G: -1.0469 D(x):0.5778 D(G(z)): 0.5526/0.5502\n",
            "[71/100] Loss_D: 0.1683 Loss_G: -1.1383 D(x):0.5928 D(G(z)): 0.5585/0.5570\n",
            "[72/100] Loss_D: 0.1635 Loss_G: -1.0623 D(x):0.5775 D(G(z)): 0.5618/0.5589\n",
            "[72/100] Loss_D: 0.1619 Loss_G: -1.0640 D(x):0.5922 D(G(z)): 0.5404/0.5381\n",
            "[73/100] Loss_D: 0.1645 Loss_G: -1.0757 D(x):0.5772 D(G(z)): 0.5529/0.5515\n",
            "[73/100] Loss_D: 0.1559 Loss_G: -1.0086 D(x):0.5919 D(G(z)): 0.5300/0.5286\n",
            "[74/100] Loss_D: 0.1525 Loss_G: -0.9575 D(x):0.5772 D(G(z)): 0.5294/0.5280\n",
            "[74/100] Loss_D: 0.1555 Loss_G: -1.0105 D(x):0.5923 D(G(z)): 0.5186/0.5178\n",
            "[75/100] Loss_D: 0.1578 Loss_G: -1.0082 D(x):0.5772 D(G(z)): 0.5362/0.5350\n",
            "[75/100] Loss_D: 0.1513 Loss_G: -0.9699 D(x):0.5925 D(G(z)): 0.5302/0.5289\n",
            "[76/100] Loss_D: 0.1630 Loss_G: -1.0621 D(x):0.5773 D(G(z)): 0.5572/0.5563\n",
            "[76/100] Loss_D: 0.1620 Loss_G: -1.0778 D(x):0.5929 D(G(z)): 0.5433/0.5417\n",
            "[77/100] Loss_D: 0.1648 Loss_G: -1.0800 D(x):0.5774 D(G(z)): 0.5454/0.5447\n",
            "[77/100] Loss_D: 0.1637 Loss_G: -1.0945 D(x):0.5931 D(G(z)): 0.5551/0.5541\n",
            "[78/100] Loss_D: 0.1683 Loss_G: -1.1141 D(x):0.5775 D(G(z)): 0.5689/0.5674\n",
            "[78/100] Loss_D: 0.1589 Loss_G: -1.0484 D(x):0.5933 D(G(z)): 0.5675/0.5654\n",
            "[79/100] Loss_D: 0.1597 Loss_G: -1.0260 D(x):0.5775 D(G(z)): 0.5546/0.5524\n",
            "[79/100] Loss_D: 0.1601 Loss_G: -1.0590 D(x):0.5932 D(G(z)): 0.5738/0.5712\n",
            "[80/100] Loss_D: 0.1640 Loss_G: -1.0711 D(x):0.5773 D(G(z)): 0.5680/0.5659\n",
            "[80/100] Loss_D: 0.1557 Loss_G: -1.0168 D(x):0.5929 D(G(z)): 0.5592/0.5568\n",
            "[81/100] Loss_D: 0.1657 Loss_G: -1.0822 D(x):0.5769 D(G(z)): 0.5635/0.5614\n",
            "[81/100] Loss_D: 0.1626 Loss_G: -1.0824 D(x):0.5925 D(G(z)): 0.5785/0.5771\n",
            "[82/100] Loss_D: 0.1540 Loss_G: -0.9708 D(x):0.5766 D(G(z)): 0.5529/0.5505\n",
            "[82/100] Loss_D: 0.1612 Loss_G: -1.0649 D(x):0.5923 D(G(z)): 0.5423/0.5412\n",
            "[83/100] Loss_D: 0.1635 Loss_G: -1.0660 D(x):0.5765 D(G(z)): 0.5296/0.5285\n",
            "[83/100] Loss_D: 0.1590 Loss_G: -1.0451 D(x):0.5922 D(G(z)): 0.5203/0.5199\n",
            "[84/100] Loss_D: 0.1634 Loss_G: -1.0644 D(x):0.5763 D(G(z)): 0.5348/0.5338\n",
            "[84/100] Loss_D: 0.1590 Loss_G: -1.0445 D(x):0.5924 D(G(z)): 0.5400/0.5390\n",
            "[85/100] Loss_D: 0.1520 Loss_G: -0.9506 D(x):0.5764 D(G(z)): 0.5140/0.5127\n",
            "[85/100] Loss_D: 0.1532 Loss_G: -0.9907 D(x):0.5922 D(G(z)): 0.5429/0.5416\n",
            "[86/100] Loss_D: 0.1563 Loss_G: -0.9940 D(x):0.5764 D(G(z)): 0.5320/0.5309\n",
            "[86/100] Loss_D: 0.1496 Loss_G: -0.9498 D(x):0.5921 D(G(z)): 0.5225/0.5211\n",
            "[87/100] Loss_D: 0.1624 Loss_G: -1.0590 D(x):0.5765 D(G(z)): 0.5536/0.5525\n",
            "[87/100] Loss_D: 0.1649 Loss_G: -1.1046 D(x):0.5922 D(G(z)): 0.5459/0.5449\n",
            "[88/100] Loss_D: 0.1593 Loss_G: -1.0240 D(x):0.5764 D(G(z)): 0.5423/0.5410\n",
            "[88/100] Loss_D: 0.1626 Loss_G: -1.0793 D(x):0.5924 D(G(z)): 0.5260/0.5240\n",
            "[89/100] Loss_D: 0.1675 Loss_G: -1.1077 D(x):0.5765 D(G(z)): 0.5591/0.5576\n",
            "[89/100] Loss_D: 0.1613 Loss_G: -1.0707 D(x):0.5927 D(G(z)): 0.5623/0.5603\n",
            "[90/100] Loss_D: 0.1610 Loss_G: -1.0394 D(x):0.5767 D(G(z)): 0.5715/0.5684\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTzH8tahxug1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "plt.plot(G_losses,label=\"G\")\n",
        "plt.plot(D_losses,label=\"D\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig(\"losses.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1CDu0Se3PmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evalArray=G_(torch.randn((1,200)).to(device))\n",
        "T=0.8\n",
        "evalArray[evalArray>T]=True\n",
        "evalArray[evalArray<T]=False\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.voxels(evalArray.squeeze().detach().cpu().numpy(),facecolors='red')\n",
        "fig.savefig('thresholded_T={0}.png'.format(T))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn_xvJC35A8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}